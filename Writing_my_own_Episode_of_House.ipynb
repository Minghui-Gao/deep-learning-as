{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6sZVmKzZP5Y"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Dense, LSTM, Dropout, Input\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUszhnOiZP5b"
      },
      "outputs": [],
      "source": [
        "# loading the data into file\n",
        "\n",
        "\n",
        "season1 = pd.read_csv(\"/content/season1.csv\", encoding_errors='ignore')\n",
        "season1.head()\n",
        "\n",
        "season1.to_csv('/content/season1.txt', index = False, sep=':')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/season1.txt'\n",
        "\n",
        "with open(data_dir) as f:\n",
        "    data = f.read()\n",
        "    \n",
        "data = data[10:].lower()\n",
        "\n",
        "# seperate the punchuations from the words\n",
        "punch = ['.', '[', ']', '(', ')', ';', ':', \"'\", '/', '\"', ',', '?', '*', '!', '-', '$', '%', '&', '\\n']\n",
        "\n",
        "for i in punch:    \n",
        "    data = data.replace(i, ' ' + i + ' ')\n",
        "    \n",
        "data = data.replace('\\n', '<NEWLINE>')"
      ],
      "metadata": {
        "id": "nm9BsTjVeztl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:400]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HFRuRud5psJE",
        "outputId": "d6148f2c-6699-459c-9a94-93be0c01b1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"melanie :  why are you late ?  <NEWLINE> rebecca :  you ' re not going to like the answer .  <NEWLINE> melanie :  i already know the answer .  <NEWLINE> rebecca :  i missed the bus .  <NEWLINE> melanie :  i don ' t doubt it ,  no bus stops near brad ' s .  you spent the night ,  the alarm didn ' t work .  or maybe it did .  <NEWLINE> rebecca :  i didn ' t sleep with him .  <NEWLINE> melanie :  gir\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfuX3NNHZP5d"
      },
      "outputs": [],
      "source": [
        "def get_vocab(text):\n",
        "    \n",
        "    vocab_to_int = dict()\n",
        "    int_to_vocab = dict()\n",
        "    \n",
        "    vocab = Counter()\n",
        "    for word in text.split():\n",
        "        vocab[word] += 1\n",
        "        \n",
        "    index = 0    \n",
        "    for word in vocab:\n",
        "        vocab_to_int[word] = index\n",
        "        int_to_vocab[index] = word\n",
        "        index += 1\n",
        "        \n",
        "    return vocab, vocab_to_int, int_to_vocab\n",
        "\n",
        "vocab, vocab_to_int, int_to_vocab = get_vocab(data)\n",
        "\n",
        "# converting text into int\n",
        "text_int = []\n",
        "\n",
        "for word in data.split():\n",
        "    text_int.append(vocab_to_int[word])\n",
        "    \n",
        "text_int = np.array(text_int) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4UJwcYlZP5f"
      },
      "outputs": [],
      "source": [
        "seq_len = 200\n",
        "\n",
        "def get_training_data(data, seq_len):\n",
        "    \n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    for i in range(0, len(data)-seq_len):\n",
        "        \n",
        "        x = data[i:i+seq_len]\n",
        "        y = data[i+1:i+seq_len+1]\n",
        "        \n",
        "        x_train.append(np.array(x))\n",
        "        y_train.append(np.array(y))\n",
        "        \n",
        "    return x_train, y_train\n",
        "  \n",
        "x, y = get_training_data(text_int, seq_len)\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "y = y.reshape(y.shape[0], y.shape[1], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wvaifKHZP5g"
      },
      "outputs": [],
      "source": [
        "embedding = 300\n",
        "lstm_size = 128\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "inp = Input((None,))\n",
        "\n",
        "embed = Embedding(input_dim=vocab_size, output_dim=embedding)\n",
        "lstm1 = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "lstm2 = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "lstm3 = LSTM(lstm_size, return_sequences=True, return_state=True)\n",
        "dense = Dense(vocab_size)\n",
        "\n",
        "net = embed(inp)\n",
        "net, h1, c1 = lstm1(net)\n",
        "net, h2, c2 = lstm2(net)\n",
        "net, h3, c3 = lstm3(net)\n",
        "out = dense(net)\n",
        "\n",
        "model = Model(inp, out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.optimizer.lr = 0.05\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(model.fit(x, y, batch_size=128, epochs=4, shuffle=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4nA62BgcdP4",
        "outputId": "e7524894-880b-4abd-e56c-169b6394eca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1576/1576 [==============================] - 266s 166ms/step - loss: 4.5754 - accuracy: 0.4216\n",
            "Epoch 2/4\n",
            "1576/1576 [==============================] - 262s 167ms/step - loss: 4.5340 - accuracy: 0.4396\n",
            "Epoch 3/4\n",
            "1576/1576 [==============================] - 262s 166ms/step - loss: 4.5187 - accuracy: 0.4533\n",
            "Epoch 4/4\n",
            "1576/1576 [==============================] - 262s 166ms/step - loss: 4.5355 - accuracy: 0.4601\n",
            "<keras.callbacks.History object at 0x7faface0ff10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "init_states = [Input((lstm_size,)) for i in range(6)]\n",
        "\n",
        "inference = embed(inp)\n",
        "inference, h1, c1 = lstm1(inference, initial_state=init_states[:2])\n",
        "inference, h2, c2 = lstm2(inference, initial_state=init_states[2:4])\n",
        "inference, h3, c3 = lstm3(inference, initial_state=init_states[4:6])\n",
        "inf_out = dense(inference)\n",
        "\n",
        "states = [h1, c1, h2, c2, h3, c3]\n",
        "inf_model = Model([inp]+init_states, [inf_out]+states)"
      ],
      "metadata": {
        "id": "duMKVYEjceHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwXOdBv0ZP5g"
      },
      "outputs": [],
      "source": [
        "def extract_text(length, start):\n",
        "    \n",
        "    states = [np.zeros((1, lstm_size)) for i in range(6)]\n",
        "\n",
        "    token = np.zeros((1,1))\n",
        "    token[0,0] = start\n",
        "    text = int_to_vocab[start] + ' '\n",
        "    \n",
        "    for i in range(length):\n",
        "        \n",
        "        out = inf_model.predict([token]+states)\n",
        "        word = np.argmax(out[0][0,0,:])\n",
        "        text += int_to_vocab[word] + ' '\n",
        "        states = out[1:7]\n",
        "        token[0][0] = word\n",
        "        \n",
        "    return text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YUwZ7INZP5h"
      },
      "outputs": [],
      "source": [
        "def post_process_text(text):\n",
        "    \n",
        "    punch1 = ['.', ':', '!', ';', ')', ']', '?', ',', '%']\n",
        "    for i in punch1:\n",
        "        text = text.replace(' '+i, i)\n",
        "        \n",
        "    punch2 = ['[', '(', '$']    \n",
        "    for i in punch2:\n",
        "        text = text.replace(i+' ', i)\n",
        "        \n",
        "    punch3 = [\"'\", '-']    \n",
        "    for i in punch3:\n",
        "        text = text.replace(' '+i+' ', i)\n",
        "        \n",
        "    text = text.split('<NEWLINE>')  \n",
        "    \n",
        "        \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbipriBZZP5h",
        "outputId": "2dfcc80b-3785-49e8-dc92-fdf93076eed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"melanie, know, you're not to make you. \", \" jessica: i don't think you. \", \" foreman: [maintain and but i think i'm not validated you. \", \" house: i don't want to see that. \", \" foreman: it's not pissing [pause and abusive down; you get to get that with him. \", ' chase: [he leaves.] ', ' foreman: [looking at the carinii. ', ' wilson: i have to make a habits that out? take? [wilson impression.] that why? ', \" cameron: lay on this experiencing this spending off i'll just foreman. just accidents, he's good. [pause] oh, you're not lame. \", \" cameron: [to the nurse] where's the pissing \", ' foreman: what we do? ', \" stacy: he's not insane, the is a contamination. you told me the stood or i do you know. \", ' house: you know. ', ' wilson: why ']\n"
          ]
        }
      ],
      "source": [
        "generated_text = extract_text(200, 0)\n",
        "generated_text = post_process_text(generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for x in range(len(generated_text)):\n",
        " print(generated_text[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beorkHTC8Y_K",
        "outputId": "3b2f082f-8d04-473c-cfff-f5f68901b4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "melanie, know, you're not to make you. \n",
            " jessica: i don't think you. \n",
            " foreman: [maintain and but i think i'm not validated you. \n",
            " house: i don't want to see that. \n",
            " foreman: it's not pissing [pause and abusive down; you get to get that with him. \n",
            " chase: [he leaves.] \n",
            " foreman: [looking at the carinii. \n",
            " wilson: i have to make a habits that out? take? [wilson impression.] that why? \n",
            " cameron: lay on this experiencing this spending off i'll just foreman. just accidents, he's good. [pause] oh, you're not lame. \n",
            " cameron: [to the nurse] where's the pissing \n",
            " foreman: what we do? \n",
            " stacy: he's not insane, the is a contamination. you told me the stood or i do you know. \n",
            " house: you know. \n",
            " wilson: why \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Writing my own Episode of House.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}